# OV Agent åç«¯æ”¹è¿›è®¡åˆ’

> å€Ÿé‰´ OpenAI Codex CLI æ¶æ„ï¼Œé’ˆå¯¹ `agent_backend.py` / `smart_agent.py` / `model_config.py` çš„å…·ä½“æ”¹è¿›æ–¹æ¡ˆã€‚
> æ¯é¡¹æ”¹è¿›åŒ…å«ï¼šç°çŠ¶åˆ†æã€ç›®æ ‡è®¾è®¡ã€æ¶‰åŠæ–‡ä»¶ä¸è¡Œå·ã€è¿ç§»æ­¥éª¤ã€Before/After ä»£ç ç¤ºä¾‹ã€å›å½’éªŒè¯è¦ç‚¹ã€‚

---

## P0-1: Provider æ³¨å†Œè¡¨æ¨¡å¼

### ç°çŠ¶åˆ†æ

å½“å‰ `agent_backend.py` ä¸­å­˜åœ¨ **ä¸‰æ¡å¹¶è¡Œçš„ if/elif åˆ†å‘é“¾**ï¼Œæ¯åŠ ä¸€ä¸ª Provider å¿…é¡»åŒæ­¥ä¿®æ”¹ä¸‰å¤„ï¼š

| åˆ†å‘ç‚¹ | ä½ç½® | èŒè´£ |
|--------|------|------|
| `_run_sync()` | `agent_backend.py:427-446` | éæµå¼è°ƒç”¨åˆ†å‘ |
| `_stream_async()` | `agent_backend.py:397-422` | æµå¼è°ƒç”¨åˆ†å‘ |
| `_resolve_api_key()` | `agent_backend.py:448-472` | API Key è§£æ |

åŒæ—¶ `model_config.py` ä¸­æœ‰ **å››å¼ å¹³é“ºå­—å…¸** (`AVAILABLE_MODELS`, `PROVIDER_API_KEYS`, `PROVIDER_ENDPOINTS`, `PROVIDER_DEFAULT_KEYS`) ä¹ŸæŒ‰ Provider ç»„ç»‡ä½†å½¼æ­¤ç‹¬ç«‹ï¼Œæ–°å¢æ¨¡å‹éœ€è¦åœ¨å››å¤„éƒ½åŠ ä¸€æ¡ã€‚

`OPENAI_COMPAT_BASE_URLS`ï¼ˆ`agent_backend.py:50-58`ï¼‰æ˜¯ç¬¬äº”ä¸ªéœ€è¦åŒæ­¥ç»´æŠ¤çš„æ•°æ®ç»“æ„ã€‚

**Codex å¯¹åº”æ–¹æ¡ˆ**: `ModelProviderInfo` ç»“æ„ä½“å°† `base_url`ã€`env_key`ã€`wire_api`ã€`retry_config`ã€`custom_headers` åˆå¹¶ä¸ºå•ä¸€æ³¨å†Œè®°å½•ï¼Œæ–°å¢ Provider åªéœ€ä¸€æ¡æ³¨å†Œã€‚

### ç›®æ ‡è®¾è®¡

```
model_config.py ä¸­çš„ 4 å¼ å­—å…¸ + agent_backend.py ä¸­çš„ OPENAI_COMPAT_BASE_URLS
    â†“ åˆå¹¶ä¸º
ProviderInfo æ•°æ®ç±»æ³¨å†Œè¡¨ (å•ä¸€æ•°æ®æº)
    â†“ æ¶ˆé™¤
_run_sync / _stream_async / _resolve_api_key ä¸­çš„ if/elif é“¾
    â†“ æ›¿æ¢ä¸º
provider_registry[provider_name].chat() / .stream() / .resolve_key()
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| `omicverse/utils/model_config.py` | é‡æ„ï¼š4 å¼ å­—å…¸ â†’ `ProviderInfo` æ³¨å†Œè¡¨ |
| `omicverse/utils/agent_backend.py` | é‡æ„ï¼šif/elif é“¾ â†’ æ³¨å†Œè¡¨æŸ¥æ‰¾ + Protocol åˆ†å‘ |

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: å®šä¹‰ ProviderInfo æ•°æ®ç±»ï¼ˆæ–°å¢äº `model_config.py`ï¼‰

```python
# --- Before: 4 å¼ æ•£è½çš„å­—å…¸ ---
PROVIDER_ENDPOINTS = {"openai": "https://api.openai.com/v1", ...}
PROVIDER_DEFAULT_KEYS = {"openai": "OPENAI_API_KEY", ...}
OPENAI_COMPAT_BASE_URLS = {"openai": "https://api.openai.com/v1", ...}

# --- After: å•ä¸€æ³¨å†Œè¡¨ ---
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional

class WireAPI(Enum):
    """LLM é€šä¿¡åè®®ç±»å‹"""
    CHAT_COMPLETIONS = "chat"        # /v1/chat/completions
    RESPONSES = "responses"          # /v1/responses (GPT-5)
    ANTHROPIC_MESSAGES = "anthropic" # Anthropic Messages API
    GEMINI_GENERATE = "gemini"       # Google Gemini generateContent
    DASHSCOPE = "dashscope"          # é˜¿é‡Œ DashScope
    LOCAL = "local"                  # æœ¬åœ° Python æ‰§è¡Œ

@dataclass(frozen=True)
class ProviderInfo:
    """å•ä¸ª Provider çš„å®Œæ•´é…ç½®ï¼ˆCodex ModelProviderInfo çš„ Python ç­‰ä»·ç‰©ï¼‰"""
    name: str                                    # "openai", "anthropic", ...
    display_name: str                            # "OpenAI", "Anthropic", ...
    base_url: str                                # API endpoint
    env_key: str                                 # ä¸» API key ç¯å¢ƒå˜é‡å
    wire_api: WireAPI                            # é€šä¿¡åè®®
    alt_env_keys: List[str] = field(default_factory=list)  # å¤‡ç”¨ç¯å¢ƒå˜é‡
    openai_compatible: bool = False              # æ˜¯å¦èµ° OpenAI SDK
    max_retry_attempts: int = 3
    stream_timeout_secs: int = 300
    models: dict = field(default_factory=dict)   # model_id -> description

# å…¨å±€æ³¨å†Œè¡¨
PROVIDER_REGISTRY: dict[str, ProviderInfo] = {}

def register_provider(info: ProviderInfo) -> None:
    """æ³¨å†Œä¸€ä¸ª Providerï¼ˆæ”¯æŒç”¨æˆ·åœ¨è¿è¡Œæ—¶æ‰©å±•ï¼‰"""
    PROVIDER_REGISTRY[info.name] = info

def get_provider(name: str) -> ProviderInfo:
    """æŒ‰åç§°æŸ¥æ‰¾ Provider"""
    if name not in PROVIDER_REGISTRY:
        raise ValueError(f"Unknown provider: {name}. "
                         f"Available: {list(PROVIDER_REGISTRY.keys())}")
    return PROVIDER_REGISTRY[name]
```

#### æ­¥éª¤ 2: æ³¨å†Œæ‰€æœ‰å†…ç½® Provider

```python
# æ›¿ä»£åŸæ¥åˆ†æ•£åœ¨ 4 å¼ å­—å…¸é‡Œçš„æ•°æ®
register_provider(ProviderInfo(
    name="openai",
    display_name="OpenAI",
    base_url="https://api.openai.com/v1",
    env_key="OPENAI_API_KEY",
    wire_api=WireAPI.CHAT_COMPLETIONS,
    openai_compatible=True,
    models={
        "gpt-5": "OpenAI GPT-5 (Latest)",
        "gpt-5-mini": "OpenAI GPT-5 Mini",
        "gpt-4o": "OpenAI GPT-4o",
        "gpt-4o-mini": "OpenAI GPT-4o Mini",
        # ...
    },
))

register_provider(ProviderInfo(
    name="anthropic",
    display_name="Anthropic",
    base_url="https://api.anthropic.com",
    env_key="ANTHROPIC_API_KEY",
    wire_api=WireAPI.ANTHROPIC_MESSAGES,
    models={
        "anthropic/claude-opus-4-1-20250805": "Claude Opus 4.1",
        # ...
    },
))

register_provider(ProviderInfo(
    name="zhipu",
    display_name="Zhipu AI",
    base_url="https://open.bigmodel.cn/api/paas/v4",
    env_key="ZAI_API_KEY",
    alt_env_keys=["ZHIPUAI_API_KEY"],  # å–ä»£ _resolve_api_key é‡Œçš„ or ç‰¹åˆ¤
    wire_api=WireAPI.CHAT_COMPLETIONS,
    openai_compatible=True,
    models={...},
))
# ... å…¶ä½™ Provider åŒç†
```

#### æ­¥éª¤ 3: é‡æ„ `_resolve_api_key`ï¼ˆ`agent_backend.py:448-472`ï¼‰

```python
# --- Before: 12 è¡Œ if/elif ---
def _resolve_api_key(self) -> Optional[str]:
    if self.config.api_key:
        return self.config.api_key
    provider = self.config.provider
    if provider == "openai":
        return os.getenv("OPENAI_API_KEY")
    if provider == "xai":
        return os.getenv("XAI_API_KEY")
    # ... é‡å¤ 8 æ¬¡ ...

# --- After: 4 è¡Œ ---
def _resolve_api_key(self) -> Optional[str]:
    if self.config.api_key:
        return self.config.api_key
    info = get_provider(self.config.provider)
    return os.getenv(info.env_key) or next(
        (os.getenv(k) for k in info.alt_env_keys if os.getenv(k)), None
    )
```

#### æ­¥éª¤ 4: é‡æ„ `_run_sync` å’Œ `_stream_async`ï¼ˆ`agent_backend.py:397-446`ï¼‰

```python
# --- Before: ä¸¤æ¡å¹¶è¡Œ if/elif é“¾ ---
def _run_sync(self, user_prompt: str) -> str:
    provider = self.config.provider
    if provider in OPENAI_COMPAT_BASE_URLS:
        return self._chat_via_openai_compatible(user_prompt)
    if provider == "python":
        return self._run_python_local(user_prompt)
    if provider == "anthropic":
        return self._chat_via_anthropic(user_prompt)
    # ...

# --- After: æŒ‰ wire_api åˆ†å‘ ---
# æ˜ å°„è¡¨ï¼šWireAPI â†’ (sync_method, stream_method)
_DISPATCH: dict[WireAPI, tuple[str, str]] = {
    WireAPI.CHAT_COMPLETIONS: ("_chat_via_openai_compatible", "_stream_openai_compatible"),
    WireAPI.RESPONSES:        ("_chat_via_openai_responses_dispatch", "_stream_openai_responses_dispatch"),
    WireAPI.ANTHROPIC_MESSAGES: ("_chat_via_anthropic", "_stream_anthropic"),
    WireAPI.GEMINI_GENERATE:   ("_chat_via_gemini", "_stream_gemini"),
    WireAPI.DASHSCOPE:         ("_chat_via_dashscope", "_stream_dashscope"),
    WireAPI.LOCAL:             ("_run_python_local", "_run_python_local_stream"),
}

def _run_sync(self, user_prompt: str) -> str:
    info = get_provider(self.config.provider)
    method_name = _DISPATCH[info.wire_api][0]
    return getattr(self, method_name)(user_prompt)

async def _stream_async(self, user_prompt: str):
    info = get_provider(self.config.provider)
    method_name = _DISPATCH[info.wire_api][1]
    async for chunk in getattr(self, method_name)(user_prompt):
        yield chunk
```

#### æ­¥éª¤ 5: é‡æ„ ModelConfig é™æ€æ–¹æ³•

`get_provider_from_model`ï¼ˆ`model_config.py:257-278`ï¼‰çš„ if/elif å‰ç¼€åŒ¹é…æ”¹ä¸ºéå†æ³¨å†Œè¡¨ï¼š

```python
# --- Before: å‰ç¼€ if/elif é“¾ ---
@staticmethod
def get_provider_from_model(model: str) -> str:
    if model.startswith("anthropic/"):
        return "anthropic"
    elif model.startswith(("qwq-", "qwen-")):
        return "dashscope"
    # ...

# --- After: ä»æ³¨å†Œè¡¨åæŸ¥ ---
@staticmethod
def get_provider_from_model(model: str) -> str:
    model = ModelConfig.normalize_model_id(model)
    for name, info in PROVIDER_REGISTRY.items():
        if model in info.models:
            return name
    # å‰ç¼€ fallback (å‘åå…¼å®¹æœªæ³¨å†Œçš„è‡ªå®šä¹‰æ¨¡å‹)
    for name, info in PROVIDER_REGISTRY.items():
        prefix = name + "/"
        if model.startswith(prefix):
            return name
    return "openai"  # é»˜è®¤
```

### å‘åå…¼å®¹

- ä¿ç•™ `AVAILABLE_MODELS`ã€`PROVIDER_API_KEYS` ç­‰å­—å…¸ä½œä¸º**è®¡ç®—å±æ€§**ï¼Œä» `PROVIDER_REGISTRY` ç”Ÿæˆï¼Œä¸ç ´åå¤–éƒ¨ä»£ç å¯¹è¿™äº›å­—å…¸çš„å¼•ç”¨ã€‚
- `OPENAI_COMPAT_BASE_URLS` æ”¹ä¸º `property`ï¼Œä» `openai_compatible=True` çš„ Provider åŠ¨æ€æ„å»ºã€‚

### å›å½’éªŒè¯

- [ ] æ‰€æœ‰å·²æœ‰æ¨¡å‹ä»èƒ½é€šè¿‡ `ModelConfig.is_model_supported()` éªŒè¯
- [ ] `_resolve_api_key()` å¯¹æ¯ä¸ª provider è¿”å›å€¼ä¸åŸå®ç°ä¸€è‡´
- [ ] `_run_sync()` / `_stream_async()` å¯¹æ¯ä¸ª provider çš„åˆ†å‘è·¯å¾„ä¸å˜
- [ ] æ–°å¢è‡ªå®šä¹‰ Provider åªéœ€è°ƒç”¨ `register_provider()` ä¸€æ¬¡

---

## P0-2: åˆ†ç»„ Config æ•°æ®ç±»

### ç°çŠ¶åˆ†æ

`OmicVerseAgent.__init__`ï¼ˆ`smart_agent.py:238`ï¼‰æ¥å— **15 ä¸ªå‚æ•°**ï¼Œæ··åˆäº†å››ä¸ªä¸ç›¸å…³çš„å…³æ³¨ç‚¹ï¼š

| å…³æ³¨ç‚¹ | å‚æ•° |
|--------|------|
| LLM é…ç½® | `model`, `api_key`, `endpoint` |
| åæ€ç­–ç•¥ | `enable_reflection`, `reflection_iterations`, `enable_result_review` |
| Notebook æ‰§è¡Œ | `use_notebook_execution`, `max_prompts_per_session`, `notebook_storage_dir`, `keep_execution_notebooks`, `notebook_timeout`, `strict_kernel_validation` |
| æ–‡ä»¶ç³»ç»Ÿä¸Šä¸‹æ–‡ | `enable_filesystem_context`, `context_storage_dir` |

åŠ æ–°åŠŸèƒ½ = åœ¨è¿™ä¸ªæ„é€ å‡½æ•°ä¸Šå†è¿½åŠ å‚æ•°ã€‚

**Codex å¯¹åº”æ–¹æ¡ˆ**: `Config` / `ConfigBuilder` æŒ‰èŒè´£åˆ†ç»„ï¼Œæ”¯æŒåˆ†å±‚åˆå¹¶ï¼ˆCLI > env > profile > project > defaultsï¼‰ã€‚

### ç›®æ ‡è®¾è®¡

```python
agent = ov.Agent(
    llm=LLMConfig(model="gpt-4o", api_key="sk-..."),
    reflection=ReflectionConfig(enabled=True, iterations=2),
    execution=ExecutionConfig(use_notebook=True, timeout=600),
    context=ContextConfig(enabled=True, storage_dir="~/.ovagent/context"),
)
# æˆ–è€…ä¿æŒå‘åå…¼å®¹çš„å¹³é“ºç”¨æ³•ï¼š
agent = ov.Agent(model="gpt-4o", api_key="sk-...")
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| `omicverse/utils/smart_agent.py:238-412` | é‡æ„æ„é€ å‡½æ•° |
| æ–°å¢ `omicverse/utils/agent_config.py` | æ–°å»ºé…ç½®æ¨¡å— |

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: å®šä¹‰åˆ†ç»„ Config æ•°æ®ç±»ï¼ˆæ–°æ–‡ä»¶ `agent_config.py`ï¼‰

```python
from dataclasses import dataclass, field
from typing import Optional
from pathlib import Path

@dataclass
class LLMConfig:
    """LLM è¿æ¥é…ç½®"""
    model: str = "gemini-2.5-flash"
    api_key: Optional[str] = None
    endpoint: Optional[str] = None

@dataclass
class ReflectionConfig:
    """ä»£ç åæ€ä¸ç»“æœå®¡æŸ¥é…ç½®"""
    enabled: bool = True
    iterations: int = 1          # è‡ªåŠ¨ clamp åˆ° 1-3
    result_review: bool = True

    def __post_init__(self):
        self.iterations = max(1, min(3, self.iterations))

@dataclass
class ExecutionConfig:
    """ä»£ç æ‰§è¡Œç¯å¢ƒé…ç½®"""
    use_notebook: bool = True
    max_prompts_per_session: int = 5
    storage_dir: Optional[Path] = None
    keep_notebooks: bool = True
    timeout: int = 600
    strict_kernel_validation: bool = True

@dataclass
class ContextConfig:
    """æ–‡ä»¶ç³»ç»Ÿä¸Šä¸‹æ–‡ç®¡ç†é…ç½®"""
    enabled: bool = True
    storage_dir: Optional[Path] = None

@dataclass
class AgentConfig:
    """Agent å®Œæ•´é…ç½®ï¼ˆèšåˆå„å­é…ç½®ï¼‰"""
    llm: LLMConfig = field(default_factory=LLMConfig)
    reflection: ReflectionConfig = field(default_factory=ReflectionConfig)
    execution: ExecutionConfig = field(default_factory=ExecutionConfig)
    context: ContextConfig = field(default_factory=ContextConfig)
    verbose: bool = True  # æ§åˆ¶ print è¾“å‡ºï¼ˆä¸º P1-1 åšé“ºå«ï¼‰

    @classmethod
    def from_flat_kwargs(cls, **kwargs) -> "AgentConfig":
        """ä»å¹³é“ºå…³é”®å­—å‚æ•°æ„å»ºï¼ˆå‘åå…¼å®¹ OmicVerseAgent åŸå§‹ç­¾åï¼‰"""
        return cls(
            llm=LLMConfig(
                model=kwargs.get("model", "gemini-2.5-flash"),
                api_key=kwargs.get("api_key"),
                endpoint=kwargs.get("endpoint"),
            ),
            reflection=ReflectionConfig(
                enabled=kwargs.get("enable_reflection", True),
                iterations=kwargs.get("reflection_iterations", 1),
                result_review=kwargs.get("enable_result_review", True),
            ),
            execution=ExecutionConfig(
                use_notebook=kwargs.get("use_notebook_execution", True),
                max_prompts_per_session=kwargs.get("max_prompts_per_session", 5),
                storage_dir=Path(kwargs["notebook_storage_dir"]) if kwargs.get("notebook_storage_dir") else None,
                keep_notebooks=kwargs.get("keep_execution_notebooks", True),
                timeout=kwargs.get("notebook_timeout", 600),
                strict_kernel_validation=kwargs.get("strict_kernel_validation", True),
            ),
            context=ContextConfig(
                enabled=kwargs.get("enable_filesystem_context", True),
                storage_dir=Path(kwargs["context_storage_dir"]) if kwargs.get("context_storage_dir") else None,
            ),
        )
```

#### æ­¥éª¤ 2: é‡æ„ `OmicVerseAgent.__init__`

```python
# --- Before: 15 ä¸ªå‚æ•°å¹³é“º ---
def __init__(self, model="gemini-2.5-flash", api_key=None, endpoint=None,
             enable_reflection=True, reflection_iterations=1, ...):

# --- After: æ¥å— AgentConfig æˆ–å¹³é“ºå‚æ•° ---
def __init__(self, config: Optional[AgentConfig] = None, **kwargs):
    if config is None:
        config = AgentConfig.from_flat_kwargs(**kwargs)
    self.config = config
    # åç»­å…¨éƒ¨ä» self.config.llm.model ç­‰è·¯å¾„è¯»å–
```

#### æ­¥éª¤ 3: `Agent()` å·¥å‚å‡½æ•°ä¿æŒç­¾åå…¼å®¹

```python
def Agent(model="gemini-2.5-flash", api_key=None, endpoint=None, *,
          config: Optional[AgentConfig] = None, **kwargs) -> OmicVerseAgent:
    if config is not None:
        return OmicVerseAgent(config=config)
    return OmicVerseAgent(config=AgentConfig.from_flat_kwargs(
        model=model, api_key=api_key, endpoint=endpoint, **kwargs
    ))
```

### å›å½’éªŒè¯

- [ ] `ov.Agent(model="gpt-4o", api_key="sk-...")` ä»ç„¶å¯ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰
- [ ] `ov.Agent(config=AgentConfig(...))` æ–°ç”¨æ³•å¯ç”¨
- [ ] æ‰€æœ‰å†…éƒ¨å¯¹ `self.enable_reflection` ç­‰å±æ€§çš„å¼•ç”¨æ”¹ä¸º `self.config.reflection.enabled`
- [ ] ç°æœ‰æµ‹è¯•ä¸å—å½±å“

---

## P0-3: å¼‚å¸¸å¤„ç†é‡æ„

### ç°çŠ¶åˆ†æ

ä¸‰ä¸ªå±‚é¢çš„é—®é¢˜ï¼š

**é—®é¢˜ 1: `run_async` ä¸­å¼‚å¸¸å½“æ§åˆ¶æµ**ï¼ˆ`smart_agent.py:2955-2976`ï¼‰

```python
except ValueError as e:
    # Priority 1 failed â†’ fall back
    fallback_occurred = True
except Exception as e:
    # æ‰€æœ‰å¼‚å¸¸ï¼ˆå« TypeError/KeyError ç­‰çœŸ bugï¼‰ä¹Ÿ fallback
    fallback_occurred = True
```

ä¸€ä¸ª `_run_registry_workflow` é‡Œçš„ `KeyError`ï¼ˆä»£ç  bugï¼‰è¢«é™é»˜åæ‰ï¼Œå½“ä½œ"ä»»åŠ¡å¤ªå¤æ‚"å¤„ç†ã€‚

**é—®é¢˜ 2: `__init__` ä¸­å¼‚å¸¸é™é»˜åæ‰**ï¼ˆ`smart_agent.py:281-285`ï¼‰

```python
try:
    model = ModelConfig.normalize_model_id(model)
except Exception:
    model = model  # æ‰€æœ‰å¼‚å¸¸åŒ…æ‹¬ bug éƒ½è¢«åæ‰
```

**é—®é¢˜ 3: `agent_backend.py` ä¸­ `_should_retry` è¿‡å®½åŒ¹é…**ï¼ˆ`agent_backend.py:164-170`ï¼‰

```python
transient_exception_names = ['apierror', ...]
```

`'apierror'` åŒ¹é… OpenAI SDK çš„ `APIError`ï¼ŒåŒ…å« 400 Bad Request ç­‰ä¸åº”é‡è¯•çš„å®¢æˆ·ç«¯é”™è¯¯ã€‚

**Codex å¯¹åº”æ–¹æ¡ˆ**: ç»“æ„åŒ– `CodexErr` æšä¸¾ï¼Œæ¯ä¸ªå˜ä½“æ˜ç¡® `is_retryable()`ï¼›å‘½ä»¤ç»“æœç”¨ `Result` ç±»å‹è€Œé exception-as-control-flowã€‚

### ç›®æ ‡è®¾è®¡

æ–°å»º `omicverse/utils/agent_errors.py`ï¼š

```python
class OVAgentError(Exception):
    """Agent é”™è¯¯åŸºç±»"""
    pass

class WorkflowNeedsFallback(OVAgentError):
    """å·¥ä½œæµåˆ¤æ–­éœ€è¦å‡çº§åˆ°æ›´å¤æ‚çš„ç­–ç•¥ï¼ˆä¸æ˜¯ bugï¼Œæ˜¯æ­£å¸¸æµè½¬ï¼‰"""
    pass

class ProviderError(OVAgentError):
    """LLM Provider ç›¸å…³é”™è¯¯"""
    def __init__(self, message: str, provider: str, status_code: int = 0,
                 retryable: bool = False):
        super().__init__(message)
        self.provider = provider
        self.status_code = status_code
        self.retryable = retryable

class ExecutionError(OVAgentError):
    """ä»£ç æ‰§è¡Œé”™è¯¯"""
    pass

class ConfigError(OVAgentError):
    """é…ç½®é”™è¯¯ï¼ˆAPI key ç¼ºå¤±ã€æ¨¡å‹ä¸æ”¯æŒç­‰ï¼‰"""
    pass

class SandboxDeniedError(ExecutionError):
    """æ²™ç®±/Notebook æ‰§è¡Œè¢«æ‹’ç»"""
    pass
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| æ–°å¢ `omicverse/utils/agent_errors.py` | æ–°å»ºé”™è¯¯å±‚çº§æ¨¡å— |
| `omicverse/utils/smart_agent.py:2955-2976` | `run_async` æ”¹ç”¨ `WorkflowNeedsFallback` |
| `omicverse/utils/smart_agent.py:281-285` | `__init__` ä¸­æ”¶çª„ except |
| `omicverse/utils/agent_backend.py:133-195` | `_should_retry` ç²¾ç¡®åˆ†ç±» |

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: `run_async` ä¸­åŒºåˆ† fallback ä¿¡å·å’ŒçœŸ bug

```python
# --- Before (smart_agent.py:2941-2976) ---
try:
    result = await self._run_registry_workflow(request, adata)
    return result
except ValueError as e:
    fallback_occurred = True
except Exception as e:        # â† bug ä¹Ÿè¢«åæ‰
    fallback_occurred = True

# --- After ---
try:
    result = await self._run_registry_workflow(request, adata)
    return result
except WorkflowNeedsFallback as e:
    # æ­£å¸¸æµè½¬ï¼šregistry å·¥ä½œæµåˆ¤æ–­éœ€è¦å‡çº§
    self._reporter.info(f"Registry workflow insufficient: {e}")
    fallback_occurred = True
except OVAgentError:
    raise  # Agent å·²çŸ¥é”™è¯¯ç›´æ¥ä¼ æ’­
except Exception as e:
    # æœªé¢„æœŸå¼‚å¸¸ â†’ è®°å½•ä½†ä¸é™é»˜
    logger.error("Unexpected error in registry workflow: %s", e, exc_info=True)
    raise
```

åœ¨ `_run_registry_workflow` ä¸­ï¼Œå°†"ä»»åŠ¡å¤ªå¤æ‚"çš„åˆ¤æ–­æ”¹ä¸º `raise WorkflowNeedsFallback(reason)` è€Œé `raise ValueError(reason)`ã€‚

#### æ­¥éª¤ 2: `__init__` ä¸­æ”¶çª„å¼‚å¸¸å¤„ç†

```python
# --- Before (smart_agent.py:281-285) ---
try:
    model = ModelConfig.normalize_model_id(model)
except Exception:
    model = model

# --- After ---
try:
    model = ModelConfig.normalize_model_id(model)
except (KeyError, ValueError):
    # normalize_model_id å¯èƒ½å¯¹æœªè¯†åˆ«çš„æ¨¡å‹åæŠ›å‡ºï¼Œä¿ç•™åŸå€¼
    pass
```

#### æ­¥éª¤ 3: `_should_retry` ç²¾ç¡®åˆ†ç±»

```python
# --- Before (agent_backend.py:164-170) ---
transient_exception_names = [
    'timeout', 'connection', 'ratelimit', 'serviceunavailable',
    'apierror', ...  # â† apierror å¤ªå®½
]

# --- After ---
# æ˜ç¡®å¯é‡è¯•çš„ SDK å¼‚å¸¸ç±»å
_RETRYABLE_EXCEPTION_NAMES = {
    'timeout', 'connection', 'ratelimit', 'serviceunavailable',
    'throttl', 'unavailable', 'overload', 'internalservererror',
}
# æ˜ç¡®ä¸å¯é‡è¯•çš„
_NON_RETRYABLE_EXCEPTION_NAMES = {
    'authenticationerror', 'permissiondenied', 'notfound',
    'badrequest', 'invalidrequest', 'validationerror',
}

def _should_retry(exc: Exception) -> bool:
    exc_type_name = type(exc).__name__.lower()
    if any(k in exc_type_name for k in _NON_RETRYABLE_EXCEPTION_NAMES):
        return False
    if any(k in exc_type_name for k in _RETRYABLE_EXCEPTION_NAMES):
        return True
    # å¯¹ APIError ç±»ï¼šæ£€æŸ¥ status_code å±æ€§
    status = getattr(exc, 'status_code', None) or getattr(exc, 'code', None)
    if isinstance(status, int):
        if status == 429 or status >= 500:
            return True
        if 400 <= status < 500:
            return False
    # ... å…¶ä½™é€»è¾‘ä¸å˜
```

### å›å½’éªŒè¯

- [ ] `_run_registry_workflow` æŠ› `WorkflowNeedsFallback` æ—¶æ­£ç¡® fallback åˆ° Priority 2
- [ ] `_run_registry_workflow` æŠ› `TypeError` æ—¶ä¸å†è¢«åæ‰ï¼Œä¼ æ’­ç»™è°ƒç”¨è€…
- [ ] OpenAI `BadRequestError`ï¼ˆ400ï¼‰ä¸å†è¢«é‡è¯•
- [ ] OpenAI `RateLimitError`ï¼ˆ429ï¼‰ä»ç„¶è¢«é‡è¯•

---

## P1-1: Event/Reporter æ›¿ä»£ print

### ç°çŠ¶åˆ†æ

`smart_agent.py` ä¸­æœ‰ **76 å¤„ `print()` è°ƒç”¨**ï¼Œåˆ†å¸ƒåœ¨ `__init__`ï¼ˆ16 å¤„ï¼‰ã€`run_async`ï¼ˆçº¦ 30 å¤„ï¼‰å’Œå„å·¥ä½œæµæ–¹æ³•ä¸­ã€‚è¿™å¯¼è‡´ï¼š

1. **æµ‹è¯•æ— æ³•é™é»˜è¿è¡Œ**ï¼šCI æ—¥å¿—è¢«å¤§é‡ emoji è¾“å‡ºæ±¡æŸ“
2. **æ— æ³•é›†æˆåˆ°éç»ˆç«¯ç¯å¢ƒ**ï¼šJupyter notebookã€Web UIã€API æœåŠ¡æ— æ³•è‡ªå®šä¹‰è¾“å‡º
3. **æ— ç»“æ„åŒ–æ—¥å¿—**ï¼šæ— æ³•æœºå™¨è§£æè¾“å‡º

**Codex å¯¹åº”æ–¹æ¡ˆ**: `deny(clippy::print_stdout)`ï¼Œæ‰€æœ‰è¾“å‡ºèµ°ç»“æ„åŒ–äº‹ä»¶é€šé“ï¼Œç”±å‰ç«¯ï¼ˆTUI/CLIï¼‰å†³å®šå¦‚ä½•æ¸²æŸ“ã€‚

### ç›®æ ‡è®¾è®¡

```python
# æ–°æ–‡ä»¶: omicverse/utils/agent_reporter.py

from dataclasses import dataclass
from enum import Enum
from typing import Callable, Optional, Protocol

class EventLevel(Enum):
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    SUCCESS = "success"

@dataclass
class AgentEvent:
    """ç»“æ„åŒ– Agent äº‹ä»¶"""
    level: EventLevel
    message: str
    category: str = ""     # "init", "execution", "reflection", "result"
    data: dict = None      # é™„åŠ ç»“æ„åŒ–æ•°æ®

class Reporter(Protocol):
    """è¾“å‡ºæ¥å£ï¼ˆCodex äº‹ä»¶é€šé“çš„ Python ç­‰ä»·ç‰©ï¼‰"""
    def emit(self, event: AgentEvent) -> None: ...

class PrintReporter:
    """é»˜è®¤å®ç°ï¼šä¿æŒå½“å‰è¡Œä¸ºï¼ˆprint åˆ° stdoutï¼‰"""
    def emit(self, event: AgentEvent) -> None:
        icon = {"info": "â„¹", "warning": "âš ï¸", "error": "âŒ",
                "success": "âœ…", "debug": "ğŸ”"}.get(event.level.value, "")
        print(f"{icon} {event.message}")

class SilentReporter:
    """é™é»˜æ¨¡å¼ï¼šåªè®°å½•åˆ° loggerï¼ˆæµ‹è¯•/æ‰¹å¤„ç†ç”¨ï¼‰"""
    def emit(self, event: AgentEvent) -> None:
        log_fn = getattr(logger, event.level.value, logger.info)
        log_fn(event.message)

class CallbackReporter:
    """å›è°ƒæ¨¡å¼ï¼šè½¬å‘ç»™ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼ˆWeb UI/Jupyter é›†æˆç”¨ï¼‰"""
    def __init__(self, callback: Callable[[AgentEvent], None]):
        self._callback = callback

    def emit(self, event: AgentEvent) -> None:
        self._callback(event)
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| æ–°å¢ `omicverse/utils/agent_reporter.py` | æ–°å»º Reporter æ¨¡å— |
| `omicverse/utils/smart_agent.py` | æ‰€æœ‰ `print()` â†’ `self._reporter.emit()` |
| `omicverse/utils/agent_config.py` | `AgentConfig` å¢åŠ  `verbose` / `reporter` å­—æ®µ |

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: åœ¨ `AgentConfig` ä¸­åŠ å…¥ Reporter é…ç½®

```python
@dataclass
class AgentConfig:
    # ...
    verbose: bool = True
    reporter: Optional[Reporter] = None  # None = æ ¹æ® verbose è‡ªåŠ¨é€‰æ‹©
```

#### æ­¥éª¤ 2: `OmicVerseAgent.__init__` ä¸­åˆå§‹åŒ– Reporter

```python
def __init__(self, config=None, **kwargs):
    # ...
    if config.reporter:
        self._reporter = config.reporter
    elif config.verbose:
        self._reporter = PrintReporter()
    else:
        self._reporter = SilentReporter()
```

#### æ­¥éª¤ 3: é€æ­¥æ›¿æ¢ printï¼ˆç¤ºä¾‹ï¼‰

```python
# --- Before (smart_agent.py:277) ---
print(f" Initializing OmicVerse Smart Agent (internal backend)...")

# --- After ---
self._reporter.emit(AgentEvent(
    level=EventLevel.INFO,
    message="Initializing OmicVerse Smart Agent (internal backend)",
    category="init",
))

# --- Before (smart_agent.py:2948) ---
print(f"âœ… SUCCESS - Priority 1 completed successfully!")

# --- After ---
self._reporter.emit(AgentEvent(
    level=EventLevel.SUCCESS,
    message="Priority 1 completed successfully",
    category="execution",
    data={"priority": 1, "strategy": "registry"},
))
```

### å›å½’éªŒè¯

- [ ] é»˜è®¤è¡Œä¸ºï¼ˆ`verbose=True`ï¼‰è¾“å‡ºä¸å½“å‰ä¸€è‡´
- [ ] `ov.Agent(verbose=False)` å®Œå…¨é™é»˜
- [ ] `CallbackReporter` èƒ½æ¥æ”¶æ‰€æœ‰äº‹ä»¶

---

## P1-2: æ‰§è¡Œæ–¹æ³•æ‹†åˆ†

### ç°çŠ¶åˆ†æ

`_execute_generated_code`ï¼ˆ`smart_agent.py:2318-2469`ï¼‰åœ¨ 150 è¡Œå†…æ··åˆäº† **10+ ä¸ªèŒè´£**ï¼š

| è¡Œå·èŒƒå›´ | èŒè´£ |
|----------|------|
| 2331-2354 | Notebook æ‰§è¡Œåˆ†å‘ + å¤±è´¥é™çº§ |
| 2356-2360 | Legacy compile/exec |
| 2362-2365 | HVG åˆ—é‡å‘½å |
| 2367-2370 | initial_cells/genes å­˜å‚¨ |
| 2372-2381 | raw/scaled layer åˆå§‹åŒ– |
| 2383-2390 | NaN å¡«å…… |
| 2392-2420 | QC åˆ«åç”Ÿæˆ (total_counts, n_counts, n_genes_by_counts, pct_counts_mito) |
| 2421-2426 | mito/mt var åˆ— |
| 2428-2437 | pandas.cut monkey-patch |
| 2441-2445 | è¾“å‡ºç›®å½•åˆ›å»º |
| 2460 | exec() æ‰§è¡Œ |
| 2463 | doublet å½’ä¸€åŒ– |
| 2466-2467 | context æŒ‡ä»¤å¤„ç† |

**Codex å¯¹åº”æ–¹æ¡ˆ**: æ‰§è¡Œç­–ç•¥ (Sandbox / In-process) ä¸æ•°æ®å‡†å¤‡å®Œå…¨åˆ†ç¦»ï¼Œç”¨ `ToolOrchestrator` åšä¸‰é˜¶æ®µæµæ°´çº¿ã€‚

### ç›®æ ‡è®¾è®¡

æ‹†åˆ†ä¸ºä¸‰ä¸ªç‹¬ç«‹ç»„ä»¶ï¼š

```
_execute_generated_code()
    â†“ æ‹†åˆ†ä¸º
1. AnnDataPreprocessor.prepare(adata)     â†’ æ•°æ®æ ‡å‡†åŒ–
2. CodeExecutor.execute(code, adata)      â†’ æ‰§è¡Œç­–ç•¥é€‰æ‹©
3. AnnDataPostprocessor.finalize(adata)   â†’ åå¤„ç† (doublet, context)
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| `omicverse/utils/smart_agent.py:2318-2469` | æ‹†åˆ†æ–¹æ³• |
| å¯é€‰æ–°å¢ `omicverse/utils/adata_normalizer.py` | æå– adata æ ‡å‡†åŒ–é€»è¾‘ |

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: æå– AnnData é¢„å¤„ç†ï¼ˆ`smart_agent.py:2362-2445`ï¼‰

```python
# æ–°å¢å¸¸é‡ï¼ˆå–ä»£ç¡¬ç¼–ç å­—ç¬¦ä¸²ï¼‰
_QC_COLUMN_ALIASES = {
    "total_counts": lambda adata: _compute_total_counts(adata),
    "n_counts": lambda adata: adata.obs.get("total_counts", 0),
    "n_genes_by_counts": lambda adata: _compute_n_genes(adata),
}
_MITO_ALIASES = ("pct_counts_mito", "pct_counts_mt")

def _prepare_adata_for_execution(self, adata: Any) -> None:
    """åœ¨ä»£ç æ‰§è¡Œå‰æ ‡å‡†åŒ– AnnData ç»“æ„ã€‚

    ä» _execute_generated_code æå–å‡ºæ¥çš„çº¯æ•°æ®æ“ä½œï¼Œ
    ä¸æ¶‰åŠä»£ç ç¼–è¯‘æˆ–æ‰§è¡Œé€»è¾‘ã€‚
    """
    self._normalize_hvg_columns(adata)
    self._store_initial_sizes(adata)
    self._ensure_raw_and_scaled(adata)
    self._fill_missing_numeric(adata)
    self._ensure_qc_aliases(adata)
    self._normalize_mito_columns(adata)

# æ¯ä¸ªå­æ–¹æ³• 10-15 è¡Œï¼ŒèŒè´£å•ä¸€ï¼Œå¯ç‹¬ç«‹æµ‹è¯•
def _normalize_hvg_columns(self, adata):
    if hasattr(adata, "var") and adata.var is not None:
        if "highly_variable" not in adata.var.columns and "highly_variable_features" in adata.var.columns:
            adata.var["highly_variable"] = adata.var["highly_variable_features"]

def _store_initial_sizes(self, adata):
    if hasattr(adata, "uns"):
        adata.uns.setdefault("initial_cells", adata.n_obs if hasattr(adata, "n_obs") else None)
        adata.uns.setdefault("initial_genes", adata.n_vars if hasattr(adata, "n_vars") else None)

# ... å…¶ä½™å­æ–¹æ³•åŒç†
```

#### æ­¥éª¤ 2: åˆ é™¤ pandas.cut monkey-patch

```python
# --- Before (smart_agent.py:2428-2437): å…¨å±€ monkey-patch ---
if not getattr(_pd.cut, "_ov_wrapped", False):
    _orig_cut = _pd.cut
    def _safe_cut(*args, **kwargs):
        kwargs.setdefault("duplicates", "drop")
        return _orig_cut(*args, **kwargs)
    _pd.cut = _safe_cut

# --- After: åœ¨ ProactiveCodeTransformer ä¸­å¤„ç† ---
# åœ¨ transform() ä¸­åŠ ä¸€æ¡è§„åˆ™ï¼š
# æŠŠç”Ÿæˆä»£ç é‡Œçš„ pd.cut(...) è‡ªåŠ¨åŠ ä¸Š duplicates="drop" å‚æ•°
# è¿™æ ·ä¸ä¼šæ±¡æŸ“å…¨å±€ pandas
```

#### æ­¥éª¤ 3: ç®€åŒ– `_execute_generated_code`

```python
def _execute_generated_code(self, code: str, adata: Any) -> Any:
    # 1. é¢„å¤„ç†
    self._prepare_adata_for_execution(adata)

    # 2. æ‰§è¡Œ
    if self.use_notebook_execution and self._notebook_executor is not None:
        result_adata = self._execute_via_notebook(code, adata)
    else:
        result_adata = self._execute_in_process(code, adata)

    # 3. åå¤„ç†
    self._normalize_doublet_obs(result_adata)
    if self.enable_filesystem_context and self._filesystem_context:
        self._process_context_directives(code, {})

    return result_adata
```

#### æ­¥éª¤ 4: Notebook é™çº§ä¸å†é™é»˜ï¼ˆè¡”æ¥ P2-3ï¼‰

```python
def _execute_via_notebook(self, code, adata):
    try:
        return self._notebook_executor.execute(code, adata)
    except Exception as e:
        # ä¸å†é™é»˜ fallthroughï¼ŒæŠ›å‡ºæ˜ç¡®å¼‚å¸¸
        raise SandboxDeniedError(
            f"Notebook execution failed: {e}. "
            "Set use_notebook_execution=False to use in-process execution."
        ) from e
```

### å›å½’éªŒè¯

- [ ] æå–åçš„ `_prepare_adata_for_execution` å¯¹ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒ adata å˜æ¢
- [ ] åˆ é™¤ pandas.cut monkey-patch åï¼Œç”Ÿæˆä»£ç ä¸­å« `pd.cut()` ä»èƒ½æ­£ç¡®æ‰§è¡Œ
- [ ] Notebook æ‰§è¡Œå¤±è´¥ä¸å†é™é»˜é™çº§

---

## P1-3: retry å»é‡ + GPT-5 å“åº”æå–å™¨å»é‡

### ç°çŠ¶åˆ†æ

**é—®é¢˜ 1: retry å‚æ•°å±•å¼€é‡å¤ 12 æ¬¡**

`agent_backend.py` ä¸­ `_retry_with_backoff(func, max_attempts=self.config.max_retry_attempts, base_delay=self.config.retry_base_delay, factor=self.config.retry_backoff_factor, jitter=self.config.retry_jitter)` è¿™ 5 è¡Œåœ¨ä»¥ä¸‹ 12 ä¸ªä½ç½®å‡ºç°ï¼š

- è¡Œ 527-533, 602-608, 758-764, 972-978, 1082-1088, 1152-1158
- è¡Œ 1213-1219, 1321-1327, 1413-1419, 1537-1543, 1604-1610, 1678-1684

**é—®é¢˜ 2: GPT-5 Responses API æ–‡æœ¬æå–é€»è¾‘é‡å¤ 4 å¤„**

~40 è¡Œçš„åµŒå¥— if/elif æ–‡æœ¬æå–é“¾å‡ºç°åœ¨ï¼š
- `_chat_via_openai_responses`ï¼ˆSDK è·¯å¾„ï¼‰
- `_chat_via_openai_responses_http`ï¼ˆHTTP è·¯å¾„ï¼‰
- `_stream_openai_responses`ï¼ˆæµå¼è·¯å¾„ï¼‰
- HTTP 400 é‡è¯•è·¯å¾„

**é—®é¢˜ 3: Gemini æµå¼ä¼ªæµå¼**ï¼ˆ`agent_backend.py:1612-1617`ï¼‰

```python
response_list = list(response)  # æ”¶é›†å…¨éƒ¨å†é€ä¸ª yieldï¼Œå¤±å»æµå¼æ„ä¹‰
```

### ç›®æ ‡è®¾è®¡

#### retry å»é‡

```python
# --- Before: 5 è¡Œ Ã— 12 å¤„ ---
return _retry_with_backoff(
    _make_sdk_call,
    max_attempts=self.config.max_retry_attempts,
    base_delay=self.config.retry_base_delay,
    factor=self.config.retry_backoff_factor,
    jitter=self.config.retry_jitter
)

# --- After: 1 è¡Œ Ã— 12 å¤„ ---
return self._retry(_make_sdk_call)
```

å®ç°ï¼š

```python
def _retry(self, func: Callable[..., T], *args, **kwargs) -> T:
    """ä½¿ç”¨å½“å‰ config ä¸­çš„é‡è¯•å‚æ•°æ‰§è¡Œ func"""
    return _retry_with_backoff(
        func, *args,
        max_attempts=self.config.max_retry_attempts,
        base_delay=self.config.retry_base_delay,
        factor=self.config.retry_backoff_factor,
        jitter=self.config.retry_jitter,
        **kwargs,
    )
```

#### GPT-5 å“åº”æå–å™¨å»é‡

```python
def _extract_responses_api_text(self, response_or_event) -> str:
    """ä» Responses API çš„å„ç§å“åº”æ ¼å¼ä¸­æå–æ–‡æœ¬ã€‚

    ç»Ÿä¸€å¤„ç† SDK å¯¹è±¡ã€HTTP dictã€æµå¼äº‹ä»¶ä¸‰ç§æ¥æºã€‚
    """
    # 1. SDK å¯¹è±¡ (æœ‰ output å±æ€§)
    if hasattr(response_or_event, 'output'):
        for item in response_or_event.output:
            if hasattr(item, 'content'):
                for block in item.content:
                    if hasattr(block, 'text'):
                        return block.text
    # 2. HTTP dict
    if isinstance(response_or_event, dict):
        for item in response_or_event.get('output', []):
            for block in item.get('content', []):
                if block.get('type') == 'output_text':
                    return block.get('text', '')
    # 3. æµå¼ delta
    if hasattr(response_or_event, 'delta'):
        return getattr(response_or_event.delta, 'text', '') or ''
    return ''
```

#### Gemini çœŸæµå¼

```python
# --- Before ---
response_list = list(response)
for chunk in response_list:
    if hasattr(chunk, 'text') and chunk.text:
        yield chunk.text

# --- After ---
for chunk in response:  # ç›´æ¥è¿­ä»£ï¼Œä¸æ”¶é›†
    if hasattr(chunk, 'text') and chunk.text:
        yield chunk.text
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| `omicverse/utils/agent_backend.py` | å…¨å±€ï¼š12 å¤„ retry ç¼©å‡ã€4 å¤„æå–å™¨åˆå¹¶ã€Gemini æµå¼ä¿®å¤ |

### å›å½’éªŒè¯

- [ ] æ‰€æœ‰ 12 ä¸ª retry è°ƒç”¨ç‚¹è¡Œä¸ºä¸å˜
- [ ] GPT-5 SDK / HTTP / æµå¼ä¸‰æ¡è·¯å¾„è¿”å›ç›¸åŒæ–‡æœ¬
- [ ] Gemini æµå¼ç”¨æˆ·å¯ä»¥çœ‹åˆ°é€ chunk è¾“å‡ºï¼ˆä¸å†ç­‰å…¨éƒ¨å®Œæˆï¼‰

---

## P2-1: ä¸Šä¸‹æ–‡çª—å£å‹ç¼©

### ç°çŠ¶åˆ†æ

å½“å‰ `OmicVerseAgent` æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯æ— çŠ¶æ€çš„ï¼ˆ`run_async` ä¸ç»´æŠ¤å¯¹è¯å†å²ï¼‰ï¼Œä½† `_run_skills_workflow` å’Œ `_run_registry_workflow` çš„ system prompt æœ¬èº«éå¸¸å¤§ï¼ˆåŒ…å«å®Œæ•´å‡½æ•°æ³¨å†Œè¡¨ + skill æŒ‡å¯¼ï¼‰ï¼Œå·²æ¥è¿‘éƒ¨åˆ†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚

æ­¤å¤–ï¼Œ`FilesystemContextManager` è™½æä¾›äº†ä¸Šä¸‹æ–‡æŒä¹…åŒ–ï¼Œä½†æ²¡æœ‰è‡ªåŠ¨å‹ç¼©æœºåˆ¶ â€”â€” éšç€ context æ–‡ä»¶å¢é•¿ï¼Œæ³¨å…¥ system prompt çš„å†…å®¹æ— é™è†¨èƒ€ã€‚

**Codex å¯¹åº”æ–¹æ¡ˆ**: `compact.rs` åœ¨ä¸Šä¸‹æ–‡è¶…é™æ—¶è‡ªåŠ¨è°ƒç”¨ LLM åšæ‘˜è¦å‹ç¼©ã€‚

### ç›®æ ‡è®¾è®¡

æ–°å¢ `ContextCompactor` æ¨¡å—ï¼š

```
ç³»ç»Ÿ prompt ç»„è£…
    â†“
æ£€æŸ¥ token ä¼°ç®— > æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶çš„ 80%?
    â†“ æ˜¯
è°ƒç”¨ LLM å‹ç¼©: ä¿ç•™å…³é”®å‡½æ•°ç­¾å + å‹ç¼©æè¿°æ–‡æœ¬
    â†“
ç”¨å‹ç¼©åçš„ prompt æ›¿æ¢åŸå§‹ prompt
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| æ–°å¢ `omicverse/utils/context_compactor.py` | æ–°å»ºæ¨¡å— |
| `omicverse/utils/smart_agent.py` | `_setup_agent` / `_run_*_workflow` ä¸­é›†æˆ |

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: Token ä¼°ç®—å·¥å…·

```python
# context_compactor.py

import re

# ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰/ 2 å­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰
def estimate_tokens(text: str) -> int:
    """å¿«é€Ÿ token ä¼°ç®—ï¼ˆæ— éœ€ tiktoken ä¾èµ–ï¼‰"""
    # æ··åˆè¯­è¨€ä¼°ç®—
    cjk_chars = len(re.findall(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]', text))
    ascii_chars = len(text) - cjk_chars
    return cjk_chars // 2 + ascii_chars // 4

# æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ï¼ˆä» ProviderInfo è·å–æˆ–ç¡¬ç¼–ç å¸¸ç”¨å€¼ï¼‰
MODEL_CONTEXT_WINDOWS = {
    "gpt-4o": 128_000,
    "gpt-4o-mini": 128_000,
    "gpt-5": 256_000,
    "gemini-2.5-flash": 1_000_000,
    "anthropic/claude-sonnet-4-20250514": 200_000,
    "deepseek/deepseek-chat": 64_000,
    # ...
}

def get_context_window(model: str) -> int:
    return MODEL_CONTEXT_WINDOWS.get(model, 32_000)
```

#### æ­¥éª¤ 2: Compactor å®ç°

```python
class ContextCompactor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼ˆå€Ÿé‰´ Codex compact.rsï¼‰"""

    COMPACT_THRESHOLD = 0.75  # è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£çš„ 75% è§¦å‘å‹ç¼©
    MAX_COMPACT_INPUT = 20_000  # å‹ç¼©è¯·æ±‚æœ¬èº«çš„ token é™åˆ¶

    def __init__(self, llm_backend, model: str):
        self._llm = llm_backend
        self._model = model
        self._context_window = get_context_window(model)

    def needs_compaction(self, system_prompt: str, user_prompt: str) -> bool:
        total = estimate_tokens(system_prompt) + estimate_tokens(user_prompt)
        return total > self._context_window * self.COMPACT_THRESHOLD

    async def compact(self, system_prompt: str) -> str:
        """å‹ç¼© system promptï¼Œä¿ç•™å…³é”®ä¿¡æ¯"""
        prompt = (
            "Summarize the following OmicVerse function registry and skill instructions. "
            "Keep: function names, parameter signatures, prerequisite chains. "
            "Remove: verbose descriptions, examples, related functions.\n\n"
            f"{system_prompt[:self.MAX_COMPACT_INPUT * 4]}"  # æˆªæ–­åˆ° ~MAX tokens
        )
        compacted = await self._llm.run(prompt)
        return compacted
```

#### æ­¥éª¤ 3: é›†æˆåˆ°å·¥ä½œæµ

```python
# åœ¨ _run_registry_workflow / _run_skills_workflow ä¸­
if self._compactor and self._compactor.needs_compaction(system_prompt, user_prompt):
    system_prompt = await self._compactor.compact(system_prompt)
    self._reporter.emit(AgentEvent(
        level=EventLevel.INFO,
        message="System prompt compressed to fit context window",
        category="execution",
    ))
```

### å›å½’éªŒè¯

- [ ] å° prompt ä¸è§¦å‘å‹ç¼©ï¼ˆè¡Œä¸ºä¸å˜ï¼‰
- [ ] å¤§ prompt å‹ç¼©åä»åŒ…å«æ‰€æœ‰å‡½æ•°åå’Œç­¾å
- [ ] å‹ç¼©åçš„ prompt ä¸è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£é™åˆ¶

---

## P2-2: ä¼šè¯æŒä¹…åŒ–ä¸æ¢å¤

### ç°çŠ¶åˆ†æ

å½“å‰ `OmicVerseAgent` çš„æ¯æ¬¡ `run()` è°ƒç”¨æ˜¯æ— çŠ¶æ€çš„ï¼Œæ²¡æœ‰è·¨è°ƒç”¨çš„å†å²è®°å½•ã€‚`FilesystemContextManager` æä¾›äº†æ–‡ä»¶çº§åˆ«çš„ context å­˜å‚¨ï¼Œä½†æ²¡æœ‰å¯¹è¯å†å²ï¼ˆä¹‹å‰çš„ request â†’ code â†’ result è®°å½•ï¼‰ã€‚

**Codex å¯¹åº”æ–¹æ¡ˆ**: `message_history.rs` ç”¨ append-only JSONL æŒä¹…åŒ–ï¼Œæ”¯æŒ `fork_thread()` / `resume_thread()`ã€‚

### ç›®æ ‡è®¾è®¡

```python
# æ–°æ–‡ä»¶: omicverse/utils/session_history.py

@dataclass
class HistoryEntry:
    session_id: str
    timestamp: float
    request: str
    generated_code: str
    result_summary: str  # è€Œéå®Œæ•´ adata
    usage: Optional[dict]
    priority_used: int
    success: bool

class SessionHistory:
    """Append-only JSONL ä¼šè¯å†å²ï¼ˆå€Ÿé‰´ Codex message_history.rsï¼‰"""

    def __init__(self, path: Path = None):
        self._path = path or Path.home() / ".ovagent" / "history.jsonl"
        self._path.parent.mkdir(parents=True, exist_ok=True)

    def append(self, entry: HistoryEntry) -> None:
        """åŸå­å†™å…¥ä¸€æ¡è®°å½•"""
        line = json.dumps(asdict(entry), ensure_ascii=False) + "\n"
        with open(self._path, "a", encoding="utf-8") as f:
            f.write(line)

    def get_session(self, session_id: str) -> list[HistoryEntry]:
        """è·å–æŸä¸ª session çš„å®Œæ•´å†å²"""
        # ...

    def get_recent(self, n: int = 10) -> list[HistoryEntry]:
        """è·å–æœ€è¿‘ n æ¡è®°å½•"""
        # ...

    def build_context_for_llm(self, session_id: str, max_entries: int = 5) -> str:
        """ä¸º LLM æ„å»ºå†å²ä¸Šä¸‹æ–‡æ‘˜è¦"""
        entries = self.get_session(session_id)[-max_entries:]
        lines = []
        for e in entries:
            lines.append(f"[Previous request]: {e.request}")
            lines.append(f"[Result]: {'Success' if e.success else 'Failed'} - {e.result_summary}")
        return "\n".join(lines)
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| æ–°å¢ `omicverse/utils/session_history.py` | æ–°å»ºæ¨¡å— |
| `omicverse/utils/smart_agent.py` | `run_async` æœ«å°¾è¿½åŠ  history è®°å½• |
| `omicverse/utils/agent_config.py` | å¢åŠ  `history_enabled` / `history_path` é…ç½® |

### é›†æˆæ–¹å¼

```python
# åœ¨ run_async æˆåŠŸè¿”å›å‰
if self._history:
    self._history.append(HistoryEntry(
        session_id=self._session_id,
        timestamp=time.time(),
        request=request,
        generated_code=generated_code,
        result_summary=f"{result.shape[0]} cells x {result.shape[1]} genes",
        usage=self.last_usage.__dict__ if self.last_usage else None,
        priority_used=priority_used,
        success=True,
    ))
```

### å›å½’éªŒè¯

- [ ] `history_enabled=False` æ—¶æ— æ–‡ä»¶ I/Oï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
- [ ] JSONL æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼Œå¯è¢« `jq` è§£æ
- [ ] `build_context_for_llm` è¾“å‡ºå¯æ³¨å…¥ system prompt

---

## P2-3: Notebook é™çº§éœ€æ˜¾å¼ç¡®è®¤

### ç°çŠ¶åˆ†æ

`smart_agent.py:2351-2354`ï¼š

```python
except Exception as e:
    print(f"âš ï¸  Session execution failed: {e}")
    print(f"   Falling back to in-process execution...")
    # Fall through to legacy execution  â† é™é»˜é™çº§
```

ç”¨æˆ·é€‰æ‹© Notebook æ‰§è¡Œæ˜¯ä¸ºäº†éš”ç¦»å’Œå®‰å…¨ï¼Œé™é»˜é™çº§åˆ°è£¸ `exec()` è¿èƒŒäº†è¿™ä¸ªæ„å›¾ã€‚ç±»ä¼¼åœ°ï¼Œ`__init__`ï¼ˆ`smart_agent.py:385-389`ï¼‰ä¸­ Notebook åˆå§‹åŒ–å¤±è´¥ä¹Ÿä¼šé™é»˜é™çº§ã€‚

**Codex å¯¹åº”æ–¹æ¡ˆ**: æ²™ç®±é™çº§éœ€è¦ç»è¿‡ Approval å®¡æ‰¹ï¼Œä¸ä¼šè‡ªåŠ¨å‘ç”Ÿã€‚

### ç›®æ ‡è®¾è®¡

ä¸¤ç§ç­–ç•¥ï¼ˆé€šè¿‡é…ç½®é€‰æ‹©ï¼‰ï¼š

```python
class SandboxFallbackPolicy(Enum):
    RAISE = "raise"              # ä¸é™çº§ï¼Œç›´æ¥æŠ›å¼‚å¸¸
    WARN_AND_FALLBACK = "warn"   # å‘å‡ºè­¦å‘Šäº‹ä»¶åé™çº§ï¼ˆå½“å‰è¡Œä¸ºä½†æœ‰ç»“æ„åŒ–é€šçŸ¥ï¼‰
    SILENT = "silent"            # é™é»˜é™çº§ï¼ˆä¸æ¨èï¼Œä¿ç•™ç»™æç«¯å‘åå…¼å®¹åœºæ™¯ï¼‰
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| `omicverse/utils/smart_agent.py:2351-2354` | æ‰§è¡Œæ—¶é™çº§ |
| `omicverse/utils/smart_agent.py:385-389` | åˆå§‹åŒ–æ—¶é™çº§ |
| `omicverse/utils/agent_config.py` | å¢åŠ  `sandbox_fallback_policy` |

### è¯¦ç»†æ­¥éª¤

```python
# --- Before (smart_agent.py:2351-2354) ---
except Exception as e:
    print(f"âš ï¸  Session execution failed: {e}")
    print(f"   Falling back to in-process execution...")
    # Fall through

# --- After ---
except Exception as e:
    policy = self.config.execution.sandbox_fallback_policy
    if policy == SandboxFallbackPolicy.RAISE:
        raise SandboxDeniedError(
            f"Notebook execution failed: {e}. "
            "Set sandbox_fallback_policy='warn' to allow in-process fallback."
        ) from e
    elif policy == SandboxFallbackPolicy.WARN_AND_FALLBACK:
        self._reporter.emit(AgentEvent(
            level=EventLevel.WARNING,
            message=f"Notebook execution failed ({e}), falling back to in-process execution",
            category="execution",
            data={"error": str(e), "fallback": "in_process"},
        ))
        # Continue to in-process execution below
    # SILENT: no notification, just fall through
```

### é»˜è®¤å€¼é€‰æ‹©

- æ–°ç”¨æˆ·ï¼š`WARN_AND_FALLBACK`ï¼ˆä¿æŒå½“å‰è¡Œä¸ºä½†æœ‰ç»“æ„åŒ–å‘Šè­¦ï¼‰
- ç”Ÿäº§ç¯å¢ƒæ¨èï¼š`RAISE`ï¼ˆä¸å…è®¸é™é»˜é™çº§ï¼‰

### å›å½’éªŒè¯

- [ ] `WARN_AND_FALLBACK` è¡Œä¸ºä¸å½“å‰ä¸€è‡´ï¼ˆä½†é€šè¿‡ Reporter è¾“å‡ºï¼‰
- [ ] `RAISE` åœ¨ Notebook å¤±è´¥æ—¶æŠ› `SandboxDeniedError`
- [ ] `__init__` ä¸­çš„åˆå§‹åŒ–é™çº§ä¹Ÿéµå¾ªåŒä¸€ç­–ç•¥

---

## P3-1: å…±äº« ThreadPoolExecutor

### ç°çŠ¶åˆ†æ

`agent_backend.py:1264-1267`ï¼š

```python
import concurrent.futures
with concurrent.futures.ThreadPoolExecutor() as executor:
    future = executor.submit(_run_stream)
```

æ¯æ¬¡ `_run_generator_in_thread` è°ƒç”¨éƒ½åˆ›å»ºå¹¶é”€æ¯ä¸€ä¸ª `ThreadPoolExecutor`ï¼Œåœ¨é«˜é¢‘æµå¼è°ƒç”¨ä¸‹é€ æˆçº¿ç¨‹åˆ›å»º/é”€æ¯å¼€é”€ã€‚

### ç›®æ ‡è®¾è®¡

```python
# --- Before: æ¯æ¬¡æ–°å»º ---
with concurrent.futures.ThreadPoolExecutor() as executor:
    future = executor.submit(_run_stream)

# --- After: ç±»çº§åˆ«å…±äº« ---
import atexit
import concurrent.futures

# æ¨¡å—çº§å•ä¾‹ï¼ˆæ‡’åˆå§‹åŒ–ï¼‰
_SHARED_EXECUTOR: Optional[concurrent.futures.ThreadPoolExecutor] = None

def _get_shared_executor() -> concurrent.futures.ThreadPoolExecutor:
    global _SHARED_EXECUTOR
    if _SHARED_EXECUTOR is None or _SHARED_EXECUTOR._shutdown:
        _SHARED_EXECUTOR = concurrent.futures.ThreadPoolExecutor(
            max_workers=4,
            thread_name_prefix="ovagent-stream",
        )
        atexit.register(_SHARED_EXECUTOR.shutdown, wait=False)
    return _SHARED_EXECUTOR
```

åœ¨ `_run_generator_in_thread` ä¸­ä½¿ç”¨ï¼š

```python
async def _run_generator_in_thread(self, generator_func):
    loop = asyncio.get_running_loop()
    queue = asyncio.Queue()
    exception_holder = []

    def _run_stream():
        try:
            for item in generator_func():
                asyncio.run_coroutine_threadsafe(queue.put(item), loop)
        except Exception as exc:
            exception_holder.append(exc)
        finally:
            asyncio.run_coroutine_threadsafe(queue.put(None), loop)

    executor = _get_shared_executor()
    executor.submit(_run_stream)  # ä¸å† with ... asï¼Œä¸é”€æ¯

    while True:
        chunk = await queue.get()
        if chunk is None:
            break
        yield chunk

    if exception_holder:
        raise exception_holder[0]
```

### æ¶‰åŠæ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ |
|------|---------|
| `omicverse/utils/agent_backend.py:1264-1278` | æ›¿æ¢ä¸ºå…±äº« executor |

### å›å½’éªŒè¯

- [ ] è¿ç»­å¤šæ¬¡æµå¼è°ƒç”¨ä¸å†åˆ›å»ºæ–°çº¿ç¨‹æ± 
- [ ] è¿›ç¨‹é€€å‡ºæ—¶ executor æ­£å¸¸ shutdownï¼ˆ`atexit`ï¼‰
- [ ] å¼‚å¸¸ä»ç„¶æ­£ç¡®ä¼ æ’­

---

## é™„å½•: é¢å¤–æ¸…ç†é¡¹

### åˆ é™¤æ­»ä»£ç 

| ä½ç½® | å†…å®¹ |
|------|------|
| `smart_agent.py:3010-3060` | `run_async_LEGACY` æ–¹æ³•ï¼ˆdocstring æ˜ç¡®è¯´"ä¸å†ä½¿ç”¨"ï¼‰ |
| `smart_agent.py:2874-2893` | `run_async` docstring ä¸­çš„æ€§èƒ½å£°æ˜ï¼ˆ"60-70% faster"ã€"~2-3 seconds"ï¼‰ |

### åˆ é™¤ `usage_breakdown` é‡å¤é‡ç½®

```python
# å‡ºç°åœ¨å¤šå¤„çš„ç›¸åŒå­—å…¸å­—é¢é‡
self.last_usage_breakdown = {
    'generation': None,
    'reflection': [],
    'review': [],
    'total': None
}
# â†’ æå–ä¸º
def _reset_usage_tracking(self):
    self.last_usage = None
    self.last_usage_breakdown = UsageBreakdown()
```

### `reasoning={"effort": "high"}` å¯é…ç½®åŒ–

```python
# --- Before: ç¡¬ç¼–ç åœ¨ 4 å¤„ ---
reasoning={"effort": "high"}

# --- After: ä» config è¯»å– ---
reasoning={"effort": self.config.llm.reasoning_effort}

# åœ¨ LLMConfig ä¸­æ·»åŠ 
@dataclass
class LLMConfig:
    # ...
    reasoning_effort: str = "high"  # "low" | "medium" | "high"
```

---

## å®æ–½é¡ºåºå»ºè®®

```
Phase 1 (P0): åŸºç¡€æ¶æ„
  â”œâ”€ P0-1: Provider æ³¨å†Œè¡¨   â† æœ€å¤§å»é‡æ”¶ç›Š
  â”œâ”€ P0-2: Config æ•°æ®ç±»     â† ä¸ºåç»­æ”¹åŠ¨æä¾›é…ç½®æ”¯ç‚¹
  â””â”€ P0-3: å¼‚å¸¸å¤„ç†å±‚çº§      â† æ¶ˆé™¤é™é»˜ bug åæ‰

Phase 2 (P1): è´¨é‡æ”¹è¿›
  â”œâ”€ P1-1: Event/Reporter    â† ä¾èµ– P0-2 çš„ verbose é…ç½®
  â”œâ”€ P1-2: æ‰§è¡Œæ–¹æ³•æ‹†åˆ†      â† ç‹¬ç«‹ï¼Œå¯å¹¶è¡Œæ¨è¿›
  â””â”€ P1-3: retry + æå–å™¨å»é‡ â† ä¾èµ– P0-1 çš„ Provider æ³¨å†Œè¡¨

Phase 3 (P2): é«˜çº§ç‰¹æ€§
  â”œâ”€ P2-1: ä¸Šä¸‹æ–‡å‹ç¼©        â† ä¾èµ– P1-3 çš„å…±äº« LLM è°ƒç”¨
  â”œâ”€ P2-2: ä¼šè¯æŒä¹…åŒ–        â† ç‹¬ç«‹
  â””â”€ P2-3: Notebook é™çº§ç­–ç•¥  â† ä¾èµ– P0-2 / P0-3

Phase 4 (P3): ä¼˜åŒ–
  â””â”€ P3-1: å…±äº«çº¿ç¨‹æ±          â† ç‹¬ç«‹ï¼Œä½é£é™©
```
